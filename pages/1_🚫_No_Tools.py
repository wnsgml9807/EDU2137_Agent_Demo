import streamlit as st
from langchain_core.messages import HumanMessage, AIMessage # AIMessageChunk ì œê±°
from langchain.memory import ConversationBufferMemory
from langchain.chains import LLMChain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from dotenv import load_dotenv
import os
import uuid
import asyncio
import time # time ëª¨ë“ˆ ì¶”ê°€

# .env íŒŒì¼ ë¡œë“œ (íŒŒì¼ì´ ì¡´ì¬í•  ê²½ìš°)
load_dotenv()

# --- LLM ì„¤ì • (Claude ì‚¬ìš© ë° dotenv í™œìš© - í˜ì´ì§€ 3 ê¸°ì¤€) ---
try:
    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
    anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")
    if not anthropic_api_key:
        # secrets.toml ì—ì„œë„ ì°¾ì•„ë³´ê¸°
        anthropic_api_key = st.secrets.get("ANTHROPIC_API_KEY")

    if not anthropic_api_key:
        raise ValueError("ANTHROPIC_API_KEYê°€ í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” secrets.tomlì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # Claude ëª¨ë¸ ì‚¬ìš©
    from langchain_anthropic import ChatAnthropic
    llm = ChatAnthropic(
        model="claude-3-5-sonnet-latest",
        temperature=0.7,
        api_key=anthropic_api_key,
        # streaming=True # LLMChainì—ì„œ streamì„ ì‚¬ìš©í•˜ë ¤ë©´ ì´ ì˜µì…˜ì€ ì œê±°í•˜ê±°ë‚˜, chainì˜ stream ë©”ì†Œë“œë¥¼ í™œìš©í•´ì•¼ í•©ë‹ˆë‹¤.
    )
except ValueError as e: # ëª…ì‹œì  ì˜¤ë¥˜ ì²˜ë¦¬
    st.error(e)
    st.stop()
except Exception as e: # ê¸°íƒ€ ì˜¤ë¥˜
    st.error(f"LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    st.stop()

# --- í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë° ë©”ëª¨ë¦¬ ì„¤ì • ---
# í˜ì´ì§€ë³„ ê³ ìœ  ë©”ëª¨ë¦¬ í‚¤
MEMORY_KEY = "no_tools_memory"
if MEMORY_KEY not in st.session_state:
    st.session_state[MEMORY_KEY] = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

memory = st.session_state[MEMORY_KEY]

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (ë©”ëª¨ë¦¬ í¬í•¨)
prompt_template = ChatPromptTemplate.from_messages(
    [
        ("system", "ë‹¹ì‹ ì€ ì‚¬ìš©ìë¥¼ ë•ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€ë‹µí•˜ê³  ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•©ë‹ˆë‹¤. \
         **í•˜ì§€ë§Œ ë‹¹ì‹ ì€ ì™¸ë¶€ ë„êµ¬ê°€ ì—†ìœ¼ë¯€ë¡œ, í˜„ì¬ ë‚ ì”¨ë‚˜ íŠ¹ì • ì¥ì†Œì˜ ì‹¤ì‹œê°„ ì •ë³´ì™€ ê°™ì€ ìµœì‹  ì •ë³´ëŠ” ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.** \
         ì¶œë ¥ì€ ì´ëª¨ì§€ì™€ ë§ˆí¬ë‹¤ìš´ì„ ì ê·¹ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ ì‚¬ìš©ìê°€ ì‰½ê²Œ ì½ì„ ìˆ˜ ìˆë„ë¡ í•˜ì„¸ìš”. \
         ì˜¤ì§ ë‹¹ì‹ ì´ í•™ìŠµí•œ ë‚´ìš©ê³¼ ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œë§Œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."),
        MessagesPlaceholder(variable_name="chat_history"), # ë©”ëª¨ë¦¬ ë³€ìˆ˜
        ("human", "{input}"), # ì‚¬ìš©ì ì…ë ¥ ë³€ìˆ˜
    ]
)

# LLMChain ìƒì„±
chain = LLMChain(llm=llm, prompt=prompt_template, memory=memory)


# --- Streamlit UI ì„¤ì • ---
st.title("ë„êµ¬ ì—†ëŠ” AI ì±—ë´‡ğŸš«")
st.write("í•™ìŠµëœ ì •ë³´ì™€ í”„ë¡¬í”„íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëŒ€ë‹µí•©ë‹ˆë‹¤. ì™¸ë¶€ ë„êµ¬ëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# --- ì±„íŒ… ê¸°ë¡ ê´€ë¦¬ (í‘œì‹œìš© ë¦¬ìŠ¤íŠ¸ ì¶”ê°€) ---
DISPLAY_MESSAGES_KEY = "no_tools_display_messages_v6"
if DISPLAY_MESSAGES_KEY not in st.session_state:
    st.session_state[DISPLAY_MESSAGES_KEY] = []

# --- íƒ€ì´í•‘ íš¨ê³¼ ì œë„ˆë ˆì´í„° --- 
def typing_effect_generator(text: str, speed: float = 0.01):
    for char in text:
        yield char
        time.sleep(speed)

# --- ë Œë”ë§ í•¨ìˆ˜ ì •ì˜ --- 
def render_message_data(msg_data, is_new=False): # is_new ì¸ì ì¶”ê°€
    role = msg_data.get("role")
    content = msg_data.get("content")
    if role == "user":
        with st.chat_message("user"):
            st.markdown(content)
    elif role == "assistant":
        with st.chat_message("assistant"):
             if content:
                 # is_new í”Œë˜ê·¸ëŠ” í˜¸ì¶œí•˜ëŠ” ê³³ì—ì„œ write_stream ì‚¬ìš© ì—¬ë¶€ ê²°ì •
                 # ì—¬ê¸°ì„œëŠ” í•­ìƒ markdown (ì €ì¥ëœ ê¸°ë¡ í‘œì‹œìš©)
                 st.markdown(content)

# --- ì´ì „ ëŒ€í™” ê¸°ë¡ í‘œì‹œ ---
for msg_data in st.session_state[DISPLAY_MESSAGES_KEY]:
    render_message_data(msg_data, is_new=False) # ì´ì „ ê¸°ë¡ì€ is_new=False

# --- ì‚¬ìš©ì ì…ë ¥ ë° AI ì‘ë‹µ ì²˜ë¦¬ (ì±—ë´‡ 2ì™€ ë™ì¼ ë¡œì§) ---
if user_input := st.chat_input("ë‚´ì¼ ì‹ ì´Œì—ì„œ í”¼í¬ë‹‰ì„ í•  ê³„íšì´ì•¼. ë‚ ì”¨ì™€ ë§›ì§‘ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³„íšì„ ì„¸ì›Œì¤˜"):
    # 1. ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ë° ì¦‰ì‹œ ë Œë”ë§
    user_msg = {"role": "user", "content": user_input}
    st.session_state[DISPLAY_MESSAGES_KEY].append(user_msg)
    render_message_data(user_msg) # is_new=Falseë¡œ ì¦‰ì‹œ ë§ˆí¬ë‹¤ìš´ ë Œë”ë§
    # ë©”ëª¨ë¦¬ëŠ” invoke ì‹œì ì— ì—…ë°ì´íŠ¸ë¨

    # 2. AI ì‘ë‹µ ìƒì„± ë° ì¦‰ì‹œ ë Œë”ë§ (íƒ€ì´í•‘ íš¨ê³¼)
    with st.chat_message("assistant"):
            try:
                # invoke ì‚¬ìš©í•˜ì—¬ ì „ì²´ ì‘ë‹µ ë°›ê¸°
                response = chain.invoke({"input": user_input})
                final_response_content = response.get('text', response.get('response', "ì‘ë‹µ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."))
                if not isinstance(final_response_content, str):
                     final_response_content = str(final_response_content)
                
                # íƒ€ì´í•‘ íš¨ê³¼ë¡œ ì¦‰ì‹œ ë Œë”ë§í•˜ê³  ìµœì¢… ë‚´ìš© ì €ì¥
                if final_response_content:
                     # st.write_stream í˜¸ì¶œí•˜ì—¬ ì¦‰ì‹œ ë Œë”ë§
                     displayed_response = st.write_stream(typing_effect_generator(final_response_content))
                     # ìµœì¢… ë¬¸ìì—´ ì €ì¥
                     st.session_state[DISPLAY_MESSAGES_KEY].append({"role": "assistant", "content": displayed_response})
                     # ë©”ëª¨ë¦¬ ìë™ ì¶”ê°€
                
            except Exception as e:
                 # ì˜¤ë¥˜ ë©”ì‹œì§€ ë Œë”ë§ ë° ì €ì¥
                 error_message = f"LLM ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
                 st.error(error_message) 
                 st.session_state[DISPLAY_MESSAGES_KEY].append({"role": "assistant", "content": error_message})

    # rerun ì œê±°

# --- AI ì‘ë‹µ ìƒì„± ë³„ë„ íŠ¸ë¦¬ê±° ë¡œì§ ì œê±° ---
